# 10.4 锁的特性和死锁

通常锁有三种作用，理解它们可以帮助你更好的理解锁。

* 锁可以避免丢失更新。如果你回想我们之前在kalloc.c中的例子，丢失更新是指我们丢失了对于某个内存page在kfree函数中的更新。如果没有锁，在出现race condition的时候，内存page不会被加到freelist中。但是加上锁之后，我们就不会丢失这里的更新。
* 锁可以打包多个操作，使它们具有原子性。我们之前介绍了加锁解锁之间的区域是critical section，在critical section的所有操作会都会作为一个原子操作执行。
* 锁可以维护共享数据结构的不变性。共享数据结构如果不被任何进程修改的话是会保持不变的。如果某个进程acquire了锁并且做了一些更新操作，共享数据的不变性暂时会被破坏，但是在release锁之后，数据的不变性又恢复了。你们可以回想一下之前在kfree函数中的freelist数据，所有的free page都在一个单链表上。但是在kfree函数中，这个单链表的head节点会更新。freelist并不太复杂，对于一些更复杂的数据结构可能会更好的帮助你理解锁的作用。

![](<../.gitbook/assets/image (679).png>)

即使是前面介绍的kfree函数这么一个简单的场景，上面的这些锁的作用都有体现。

接下来我们再来看一下锁可能带来的一些缺点。我们已经知道了锁可以被用来解决一些正确性问题，例如避免race condition。但是不恰当的使用锁，可能会带来一些锁特有的问题。最明显的一个例子就是死锁（Deadlock）。

一个死锁的最简单的场景就是：首先acquire一个锁，然后进入到critical section；在critical section中，再acquire同一个锁；第二个acquire必须要等到第一个acquire状态被release了才能继续执行，但是不继续执行的话又走不到第一个release，所以程序就一直卡在这了。这就是一个死锁。

![](<../.gitbook/assets/image (734).png>)

这是死锁的一个最简单的例子，XV6会探测这样的死锁，如果XV6看到了同一个进程多次acquire同一个锁，就会触发一个panic。

当有多个锁的时候，场景会更加有趣。假设现在我们有两个CPU，一个是CPU1，另一个是CPU2。CPU1执行rename将文件d1/x移到d2/y，CPU2执行rename将文件d2/a移到d1/b。这里CPU1将文件从d1移到d2，CPU2正好相反将文件从d2移到d1。我们假设我们按照参数的顺序来acquire锁，那么CPU1会先获取d1的锁，如果程序是真正的并行运行，CPU2同时也会获取d2的锁。之后CPU1需要获取d2的锁，这里不能成功，因为CPU2现在持有锁，所以CPU1会停在这个位置等待d2的锁释放。而另一个CPU2，接下来会获取d1的锁，它也不能成功，因为CPU1现在持有锁。这也是死锁的一个例子，有时候这种场景也被称为deadly embrace。这里的死锁就没那么容易探测了。

![](<../.gitbook/assets/image (869).png>)

这里的解决方案是，如果你有多个锁，你需要对锁进行排序，所有的操作都必须以相同的顺序获取锁。

![](<../.gitbook/assets/image (868).png>)

所以对于一个系统设计者，你需要确定对于所有的锁对象的全局的顺序。例如在这里的例子中我们让d1一直在d2之前，这样我们在rename的时候，总是先获取排序靠前的目录的锁，再获取排序靠后的目录的锁。如果对于所有的锁有了一个全局的排序，这里的死锁就不会出现了。

不过在设计一个操作系统的时候，定义一个全局的锁的顺序会有些问题。如果一个模块m1中方法g调用了另一个模块m2中的方法f，那么m1中的方法g需要知道m2的方法f使用了哪些锁。因为如果m2使用了一些锁，那么m1的方法g必须集合f和g中的锁，并形成一个全局的锁的排序。这意味着在m2中的锁必须对m1可见，这样m1才能以恰当的方法调用m2。

但是这样又违背了代码抽象的原则。在完美的情况下，代码抽象要求m1完全不知道m2是如何实现的。但是不幸的是，具体实现中，m2内部的锁需要泄露给m1，这样m1才能完成全局锁排序。所以当你设计一些更大的系统时，锁使得代码的模块化更加的复杂了。

> 学生提问：有必要对所有的锁进行排序吗？
>
> Frans教授：在上面的例子中，这取决于f和g是否共用了一些锁。如果你看XV6的代码，你可以看到会有多种锁的排序，因为一些锁与其他的锁没有任何关系，它们永远也不会在同一个操作中被acquire。如果两组锁不可能在同一个操作中被acquire，那么这两组锁的排序是完全独立的。所以没有必要对所有的锁进行一个全局的排序，但是所有的函数需要对共同使用的一些锁进行一个排序。







# 10.4 锁的特性和死锁

本节课分析了锁的主要功能及其潜在问题，尤其是死锁问题，并探讨了应对多个锁引发的死锁的解决方案。

## 1. 锁的三种主要功能

锁在多核环境中的作用主要有以下三点：

- **避免更新丢失**：锁可以防止共享数据在并发访问中丢失更新。例如，在`kfree`函数中，没有锁保护时，多个CPU核会导致内存page无法成功添加到`freelist`中。
- **保证原子性操作**：在critical section（临界区）内的操作会被视作一个不可分割的原子操作，不会被其他进程中断或交织执行。
- **维护数据结构的不变性**：锁可以在更新共享数据结构时暂时允许不变性被破坏，但在锁释放后恢复不变性。例如在`kfree`的`freelist`单链表中，锁保护可以确保freelist在每次更新后保持正确的链表结构。

## 2. 锁的缺点：死锁

尽管锁可以解决race condition，但不当的使用会导致死锁。死锁是指进程因循环等待资源而无法继续执行的状态。

### 死锁的简单例子

==最简单的死锁情况是单个进程在critical section内再次尝试获取同一把锁。==进程必须等待自己释放锁才能继续执行，但由于它已在critical section内，导致程序卡住，发生死锁。

### 多锁引发的死锁

==当多个锁同时被不同的进程获取时，死锁问题更复杂。==例如：

- **场景**：==两个CPU核分别调用`rename`操作来交换两个目录中的文件。CPU1和CPU2分别获取目录`d1`和`d2`的锁，但因为对方持有自己需要的锁，双方都被迫等待对方释放锁，形成循环等待，导致死锁。==

### 死锁的解决方法

==为避免多锁的死锁问题，解决方案是**锁的顺序控制**==：

- **全局锁排序**：==对所有锁定义一个全局顺序，操作时按顺序获取锁。==例如，让目录`d1`始终在`d2`之前被加锁。这样在`rename`操作中，始终先获取`d1`的锁，再获取`d2`的锁，避免循环等待。

## 3. 全局锁排序的复杂性与模块化的挑战

==设计操作系统时，实施全局锁排序可能会破坏模块化的原则==：

- **模块间的锁依赖问题**：例如，如果模块`m1`的函数`g`调用了模块`m2`的函数`f`，而`f`有自己的锁，那么模块`m1`需要了解模块`m2`使用的锁，以便对整个锁的集合进行排序。这使得模块`m2`的锁必须对模块`m1`公开，违反了模块化的封装性。

## 4. 并非所有锁都需要排序

> **学生提问**：是否有必要对所有锁进行排序？
>
> **Frans教授**：==不需要。只需对可能在同一操作中被同时acquire的锁进行排序。不同的锁组之间若不会被同一操作同时使用，则这些锁可以独立排序。==

------

总结来看，锁的三大功能使得它在并发编程中不可或缺，但使用锁时需谨慎以避免死锁。通过对共享锁的排序和谨慎管理，程序设计可以在数据安全与模块化之间取得平衡，从而在提升并发性的同时保持系统的稳定性和可维护性。







## 拓展：

### 问题：critical section什么意思，详细解释一下

**Critical Section（临界区）** 是指多线程或多进程程序中需要**互斥访问的代码块**，通常用于访问共享资源，如全局变量、文件、数据库等。临界区的存在确保了同一时间只有一个线程或进程能够访问这些共享资源，从而避免了竞争条件（Race Condition）。

#### 为什么需要临界区

当多个线程或进程对同一个共享资源进行操作时，可能会导致数据不一致问题。临界区的目的是让这些线程或进程在访问共享资源时**互斥**，确保只有一个线程或进程可以进入临界区操作资源，其他线程或进程必须等待，直到该资源操作完成。

#### 临界区的工作原理

临界区的访问是通过**同步机制**来控制的，比如互斥锁（Mutex）、信号量（Semaphore）、自旋锁（Spinlock）等。使用这些同步机制可以确保：

- 在某个线程进入临界区后，其他线程必须等待直到该线程退出临界区。
- 只有在临界区空闲时，其他线程才能进入。

这样，通过在共享资源的访问前后加锁和解锁操作，可以保证临界区代码的安全访问。

#### 临界区的示例

一个简单的示例是银行账户的余额更新操作。假设两个线程需要分别对账户余额进行存款和取款操作：

```c
int balance = 100; // 共享资源

void deposit(int amount) {
    balance = balance + amount;
}

void withdraw(int amount) {
    balance = balance - amount;
}
```

如果没有同步机制保护 `balance`，多个线程同时调用 `deposit` 和 `withdraw` 时就可能发生竞争条件，导致余额出现错误。为了解决这个问题，我们可以将 `balance` 的访问操作放入临界区中，使用互斥锁保护：

```c
pthread_mutex_t lock;

void deposit(int amount) {
    pthread_mutex_lock(&lock);    // 加锁进入临界区
    balance = balance + amount;
    pthread_mutex_unlock(&lock);  // 解锁离开临界区
}

void withdraw(int amount) {
    pthread_mutex_lock(&lock);    // 加锁进入临界区
    balance = balance - amount;
    pthread_mutex_unlock(&lock);  // 解锁离开临界区
}
```

在上述代码中，`pthread_mutex_lock` 和 `pthread_mutex_unlock` 分别用于进入和退出临界区，确保同一时间只有一个线程可以对 `balance` 进行操作。

#### 临界区的特性

- **互斥性**：在任何时候，只有一个线程可以进入临界区，其他线程必须等待。
- **原子性**：在临界区中执行的操作要么全部完成，要么全部不执行，确保共享资源的一致性。
- **最小化时间**：临界区的操作应尽量简单和快速，以减少锁定时间，避免阻塞其他线程的访问。

#### 进入临界区的同步机制

多线程编程中，常用的同步机制用于确保进入临界区的互斥性，包括：

1. **互斥锁（Mutex）**：一种最常用的同步机制，用于锁定临界区，使得同一时间只有一个线程能够访问共享资源。
2. **自旋锁（Spinlock）**：类似于互斥锁，但等待时会忙等待。适用于临界区非常短且等待时间较少的情况。
3. **信号量（Semaphore）**：用于控制对资源的访问量。二值信号量可以模拟互斥锁，计数信号量可以允许多个线程同时访问（适用于读多写少的场景）。
4. **读写锁（Reader-Writer Lock）**：用于读多写少的场景，允许多个线程同时读取，但写入操作必须是独占的。

#### 临界区的应用场景

临界区广泛应用于需要共享资源的并发操作中，例如：

- 更新共享变量（如计数器、标志位、数据结构）
- 文件操作（防止多个线程同时写入文件导致数据不一致）
- 数据库事务操作（多个用户访问相同的数据记录）
- 设备控制（如操作传感器、硬件设备）

#### 小结

临界区是并发编程中需要保护的代码块，以避免多个线程或进程同时访问共享资源导致的数据不一致问题。通过互斥锁、信号量、自旋锁等同步机制，可以有效保护临界区，确保并发操作的正确性和数据的安全性。

------

