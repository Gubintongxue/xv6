# 10.8 自旋锁（Spin lock）的实现（二）

有关spin lock的实现，有3个细节我想介绍一下。

首先，有很多同学提问说为什么release函数中不直接使用一个store指令将锁的locked字段写为0？有人想回答一下为什么吗？

> 学生回答：因为其他的处理器可能会向locked字段写入1，或者写入0。。。

是的，可能有两个处理器或者两个CPU同时在向locked字段写入数据。这里的问题是，对于很多人包括我自己来说，经常会认为一个store指令是一个原子操作，但实际并不总是这样，这取决于具体的实现。例如，对于CPU内的缓存，每一个cache line的大小可能大于一个整数，那么store指令实际的过程将会是：首先会加载cache line，之后再更新cache line。所以对于store指令来说，里面包含了两个微指令。这样的话就有可能得到错误的结果。所以为了避免理解硬件实现的所有细节，例如整数操作不是原子的，或者向一个64bit的内存值写数据是不是原子的，我们直接使用一个RISC-V提供的确保原子性的指令来将locked字段写为0。

amoswap并不是唯一的原子指令，下图是RISC-V的手册，它列出了所有的原子指令。

![](<../.gitbook/assets/image (497).png>)

第二个细节是，在acquire函数的最开始，会先关闭中断。为什么会是这样呢？让我们回到uart.c中。我们先来假设acquire在一开始并没有关闭中断。在uartputc函数中，首先会acquire锁，如果不关闭中断会发生什么呢？

![](<../.gitbook/assets/image (618).png>)

uartputc函数会acquire锁，UART本质上就是传输字符，当UART完成了字符传输它会做什么？是的，它会产生一个中断之后会运行uartintr函数，在uartintr函数中，会获取同一把锁，但是这把锁正在被uartputc持有。如果这里只有一个CPU的话，那这里就是死锁。中断处理程序uartintr函数会一直等待锁释放，但是CPU不出让给uartputc执行的话锁又不会释放。在XV6中，这样的场景会触发panic，因为同一个CPU会再次尝试acquire同一个锁。

所以spinlock需要处理两类并发，一类是不同CPU之间的并发，一类是相同CPU上中断和普通程序之间的并发。针对后一种情况，我们需要在acquire中关闭中断。中断会在release的结束位置再次打开，因为在这个位置才能再次安全的接收中断。

第三个细节就是memory ordering。假设我们先通过将locked字段设置为1来获取锁，之后对x加1，最后再将locked字段设置0来释放锁。下面将会是在CPU上执行的指令流：

![](<../.gitbook/assets/image (430).png>)

但是编译器或者处理器可能会重排指令以获得更好的性能。对于上面的串行指令流，如果将x<-x+1移到locked<-0之后可以吗？这会改变指令流的正确性吗？

并不会，因为x和锁完全相互独立，它们之间没有任何关联。如果他们还是按照串行的方式执行，x<-x+1移到锁之外也没有问题。所以在一个串行执行的场景下是没有问题的。实际中，处理器在执行指令时，实际指令的执行顺序可能会改变。编译器也会做类似的事情，编译器可能会在不改变执行结果的前提下，优化掉一些代码路径并进而改变指令的顺序。

但是对于并发执行，很明显这将会是一个灾难。如果我们将critical section与加锁解锁放在不同的CPU执行，将会得到完全错误的结果。所以指令重新排序在并发场景是错误的。为了禁止，或者说为了告诉编译器和硬件不要这样做，我们需要使用memory fence或者叫做synchronize指令，来确定指令的移动范围。对于synchronize指令，任何在它之前的load/store指令，都不能移动到它之后。锁的acquire和release函数都包含了synchronize指令。

![](<../.gitbook/assets/image (452) (1).png>)

这样前面的例子中，x<-x+1就不会被移到特定的memory synchronization点之外。我们也就不会有memory ordering带来的问题。这就是为什么在acquire和release中都有\_\_sync\_synchronize函数的调用。

> 学生提问：有没有可能在锁acquire之前的一条指令被移到锁release之后？或者说这里会有一个界限不允许这么做？
>
> Frans教授：在这里的例子中，acquire和release都有自己的界限（注，也就是\_\_sync\_synchronize函数的调用点）。所以发生在锁acquire之前的指令不会被移到acquire的\_\_sync\_synchronize函数调用之后，这是一个界限。在锁的release函数中有另一个界限。所以在第一个界限之前的指令会一直在这个界限之前，在两个界限之间的指令会保持在两个界限之间，在第二个界限之后的指令会保持在第二个界限之后。

最后我们时间快到了，让我来总结一下这节课的内容。

首先，锁确保了正确性，但是同时又会降低性能，这是个令人失望的现实，我们是因为并发运行代码才需要使用锁，而锁另一方面又限制了代码的并发运行。

其次锁会增加编写程序的复杂性，在我们的一些实验中会看到锁，我们需要思考锁为什么在这，它需要保护什么。如果你在程序中使用了并发，那么一般都需要使用锁。如果你想避免锁带来的复杂性，可以遵循以下原则：不到万不得已不要共享数据。如果你不在多个进程之间共享数据，那么race condition就不可能发生，那么你也就不需要使用锁，程序也不会因此变得复杂。但是通常来说如果你有一些共享的数据结构，那么你就需要锁，你可以从coarse-grained lock开始，然后基于测试结果，向fine-grained lock演进。

最后，使用race detector来找到race condition，如果你将锁的acquire和release放置于错误的位置，那么就算使用了锁还是会有race。

以上就是对锁的介绍，我们之后还会介绍很多锁的内容，在这门课程的最后我们还会介绍lock free program，并看一下如何在内核中实现它。

> 学生提问：在一个处理器上运行多个线程与在多个处理器上运行多个进程是否一样？
>
> Frans教授：差不多吧，如果你有多个线程，但是只有一个CPU，那么你还是会想要特定内核代码能够原子执行。所以你还是需要有critical section的概念。你或许不需要锁，但是你还是需要能够对特定的代码打开或者关闭中断。如果你查看一些操作系统的内核代码，通常它们都没有锁的acquire，因为它们假定自己都运行在单个处理器上，但是它们都有开关中断的操作。



# 10.8 自旋锁（Spin Lock）的实现（二）

本节课详细介绍了自旋锁实现中的关键细节，包括使用原子操作、处理中断以及内存顺序管理等问题。通过这些实现细节，可以更深入地理解自旋锁在多核系统中的工作机制和挑战。

## 1. 使用原子操作实现释放锁（Release）

一个常见问题是，==为什么在`release`函数中不直接使用`store`指令将`locked`字段设为0==。主要原因如下：

- **原子性保障**：==`store`指令并不总是原子的。例如，CPU在更新`cache line`时可能会先加载再写入，导致中间状态。这种非原子操作会引发race condition，因为多个CPU可能同时写入`locked`字段。==
- **硬件原子指令**：为避免这一问题，==RISC-V提供的`amoswap`等原子指令可以确保数据交换的原子性。==不同的处理器架构可能采用内存控制器、总线控制器、或缓存一致性协议来保证数据的唯一写入。

## 2. 关闭中断以避免同CPU上锁重入

在多核系统中，除了不同CPU之间的并发，自旋锁还需要处理同一CPU上的中断与常规程序之间的并发。为此，`acquire`函数的开头关闭中断，以防止在同一CPU上产生死锁。

- **中断中的锁竞争**：==例如，在`uartputc`函数中，若不关闭中断，CPU可能在获取锁后被UART中断打断，导致`uartintr`函数再次尝试获取相同的锁，造成死锁。==
- **acquire和release的中断管理**：`acquire`在开始时关闭中断，`release`在结束时重新打开中断，确保在critical section之外才会接收中断。

## 3. 内存顺序与同步指令

在并发环境下，指令可能因编译器和硬件优化而重新排序，这种指令重排会影响锁的正确性。

- **指令重排的风险**：若编译器或处理器将critical section中的指令重排到锁外，则会破坏锁的保护性，可能导致数据不一致。因此，需要对指令顺序进行管理。
- **内存栅栏（Memory Fence）**：==在`acquire`和`release`中调用`__sync_synchronize`函数，该函数通过memory fence指令确保指令的执行顺序不被重排。这样，可以防止critical section内的操作被移到锁的边界之外。==

### 4. 内存栅栏的边界管理

> **学生提问**：是否有界限阻止指令重排至锁之外？
>
> **Frans教授**：`acquire`和`release`分别在其开头和结尾包含`__sync_synchronize`，形成明确的界限。指令不会越过这两个边界，从而确保锁的正确性。

## 5. 锁的设计对程序复杂性的影响

总结来看，自旋锁在保证并发操作的正确性方面非常重要，但也带来了性能损失和程序复杂性：

- **锁的权衡**：锁在保证正确性的同时限制了并行性能。尽管锁能避免race condition，但锁的存在也降低了代码的并发执行能力。
- **避免复杂性的原则**：如果可能，尽量避免共享数据，减少锁的使用。若必须使用锁，可以先采用粗粒度锁（coarse-grained lock），然后根据性能测试结果逐步优化到细粒度锁（fine-grained lock）。
- **Race Condition检测**：可以使用race detection工具检测程序中的潜在race condition。如果锁的acquire和release位置不当，即使使用了锁也可能导致数据冲突。

## 6. 锁在单处理器系统中的应用

> **学生提问**：在单处理器上运行多个线程与多处理器的并发有何异同？
>
> **Frans教授**：即使在单处理器上，仍然需要保护critical section的概念，通常通过开关中断来确保critical section内的原子性，而非锁的acquire。这些操作保证单核环境下的代码安全性和一致性。

------

### 总结

- **原子性、内存顺序与中断处理**：自旋锁的正确实现依赖于硬件的原子指令、内存栅栏确保指令顺序、中断的管理。
- **锁的复杂性**：尽量减少不必要的共享数据，通过测试逐步优化锁的粒度。
- **未来课程内容**：后续课程将介绍更复杂的锁和无锁编程（lock-free programming）技术。

通过这些细节理解，可以更全面地掌握自旋锁的实现原理和在并发编程中的应用。





## 拓展

### 问题：内存栅栏什么意思，详细解释

**内存栅栏（Memory Barrier）** 是一种用于多线程或多处理器系统中的机制，确保对内存的访问顺序符合程序的预期，以避免因指令重排序而引起的不可预测行为。内存栅栏通过防止特定类型的指令重排序，确保在多核 CPU 或多线程环境中访问共享内存的正确性和一致性。

在现代 CPU 中，为了优化性能，CPU 和编译器可能会重新排序指令的执行顺序，特别是对于不具有依赖关系的内存访问指令。虽然重排序能提高单线程性能，但在多线程或多处理器环境中，重排序可能导致不可预测的结果。例如，某个线程可能会读取到另一个线程尚未完成的写操作。这种情况在并发编程中尤为重要，因此需要使用内存栅栏来控制这种重排序行为。

### 内存栅栏的类型

根据作用的不同，内存栅栏一般分为以下几种类型：

1. **读栅栏（Load Barrier / Read Barrier）**
   - 读栅栏确保在栅栏之前的所有读操作完成后，才开始执行栅栏之后的读操作。
   - 防止 CPU 将栅栏后面的读操作提前到栅栏之前执行。
2. **写栅栏（Store Barrier / Write Barrier）**
   - 写栅栏确保在栅栏之前的所有写操作完成后，才开始执行栅栏之后的写操作。
   - 防止 CPU 将栅栏后面的写操作提前到栅栏之前执行。
3. **全栅栏（Full Barrier / Memory Barrier）**
   - 全栅栏同时保证了读和写操作的顺序，即在栅栏之前的所有读和写操作都必须在栅栏之后的读和写操作之前完成。
   - 防止 CPU 对读写操作进行任何重新排序。
4. **顺序栅栏（Acquire-Release Barrier）**
   - 顺序栅栏（也称获取-释放栅栏）常用于加锁和解锁操作中。在进入关键区（acquire）时使用加载栅栏，在离开关键区（release）时使用存储栅栏。
   - 确保在“获取”操作（比如加锁）之前的读写不会重排到“获取”之后，同样，“释放”操作（比如解锁）之后的操作也不会被重排到“释放”之前。

### 为什么需要内存栅栏

在多核处理器或多线程程序中，内存栅栏用于确保线程间或处理器间的操作顺序，防止数据竞争（Data Race）和不一致的行为。内存栅栏的主要作用包括：

1. **防止指令重排序**：编译器和 CPU 都可能在执行指令时调整指令顺序，导致不同线程看到的执行顺序不同。内存栅栏可以防止关键的读写操作被重新排序。
2. **确保内存可见性**：内存栅栏保证了在一个线程中执行的写操作对其他线程可见。特别是在多处理器系统中，每个 CPU 都有自己的缓存，内存栅栏可以确保数据被刷新到主内存中，从而让其他 CPU 能看到最新的内存数据。
3. **保证并发正确性**：在多线程程序中，正确的同步操作（如加锁、解锁等）需要内存栅栏来确保临界区内的操作是按照程序预期顺序执行的。

### 内存栅栏的工作原理

内存栅栏在硬件层面通常通过特殊指令（如 `mfence`、`lfence`、`sfence`）实现，这些指令对处理器的缓存系统和指令执行流水线进行控制，确保指令按特定顺序执行。例如，在 x86 平台上：

- **`mfence`**：强制执行所有内存读写操作，不允许任何重排序。
- **`lfence`**：确保所有读操作在执行到 `lfence` 指令之前完成。
- **`sfence`**：确保所有写操作在执行到 `sfence` 指令之前完成。

这些指令可以让处理器在执行多线程代码时更好地管理内存一致性问题。

### 内存栅栏的使用场景

以下是一些常见的内存栅栏使用场景：

1. **双重检查锁定（Double-Checked Locking）**：
   - 在双重检查锁定模式中，确保对共享变量的读写操作顺序正确是非常重要的，通常需要使用内存栅栏来防止编译器和 CPU 的重排序。
2. **生产者-消费者模型**：
   - 在生产者线程和消费者线程之间共享数据时，生产者线程更新的数据需要确保对消费者线程可见。这时通常需要使用写栅栏来确保消费者读取到最新的数据。
3. **锁和解锁操作**：
   - 在加锁和解锁操作中使用顺序栅栏，确保加锁后的所有操作不会被重新排序到加锁之前，解锁前的操作不会被重新排序到解锁之后。

### 示例代码说明

以下是一个简单示例，展示内存栅栏如何确保顺序正确性：

```C
// 线程1
shared_var = 1;
__sync_synchronize();  // 全栅栏，确保 shared_var 写入完成后再继续

flag = 1;  // 设置一个标志位，通知线程2可以继续

// 线程2
while (flag == 0);  // 等待线程1设置标志位

__sync_synchronize();  // 全栅栏，确保读取 shared_var 前已经检测到 flag
int x = shared_var;    // 读取线程1写入的 shared_var
```

在这个例子中：

- `线程1` 先写入 `shared_var`，然后通过全栅栏 `__sync_synchronize()` 确保 `shared_var` 的写操作完成后才设置 `flag`。
- `线程2` 先检测到 `flag` 设置为 1，然后再通过全栅栏确保读取 `shared_var` 时，能够看到 `线程1` 的更新。

### 总结

内存栅栏是一种防止内存访问指令重排序的机制，确保多线程或多处理器环境中的共享内存操作符合预期的执行顺序。内存栅栏的主要作用是：

1. **防止指令重排序**。
2. **确保不同线程或处理器间的内存一致性**。
3. **保障并发操作的正确性**。

根据内存访问类型，内存栅栏分为读栅栏、写栅栏和全栅栏等多种形式，应用场景包括加锁解锁、双重检查锁定以及生产者-消费者模型等。在实际操作系统和多线程应用中，合理使用内存栅栏是确保数据一致性和程序正确性的关键。
